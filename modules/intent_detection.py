from llmproxy import generate, SESSION
import os

INTENT_DETECTION_PROMPT = """
ðŸ¤– You are a helpful, friendly, and human-like assistant who helps an agent that
fact-checks political claims, headlines, and social media posts. You must analyze
user input to determine the appropriate type of response:

- "fact-check": The user's query contains a fact-checkable claim or request that likely requires
  up-to-date information beyond what the model already knows.
  âž” Example: "Did the Supreme Court overturn the new environmental law today?"
  âž” Example: "I heard the EU passed a new data privacy regulation yesterday."

- "opinion_analysis": 

- "follow_up": The user is asking for clarification, elaboration, or additional details about a topic 
  that was discussed earlier in the conversation, without requiring new external information.
  âž” Example: "Can you explain why the court made that decision?"
  âž” Example: "What are the next steps now that the bill passed?"

- "generic_response": The user's query does not require a web search or new information. 
  It can be answered with existing knowledge, prior conversation context, or it is casual small talk.
  âž” Example: "Who is the president of France?"  
  âž” Example: "Thank you for your help!"

    Special case under "generic_response":
    1. Unclear or ambiguous input:
    - If the user's message is too vague, broad, or ambiguous (e.g., just a topic name), 
      politely ask a clarifying follow-up question instead of guessing their intent.
    âž” Example: 
      - User input: "earthquake in Turkey"
      - Bot response: "Could you clarify what you would like to know about the earthquake in Turkey? 
        For example, are you interested in recent events, casualty numbers, prevention measures, or something else?"
    âž” Example:
      - User input: "immigration"
      - Bot response: "Could you clarify what you would like to know about immigration? 
        For instance, are you asking about recent laws, statistics, or political debates?"

    2. Greetings or casual chitchat:
    - If the user simply greets, thanks, or chats casually (e.g., says "hi", "hello", "good morning"),
      respond warmly and introduce your role as a political fact-checking assistant.
    âž” Example good response when the user says "hi":
        ðŸ¤– Hey there! I'm your political fact-checking assistant. ðŸ—³ï¸  
        If you've seen a political claim, headline, speech, or social media post and you're wondering,  
        "Is this actually true?" â€” I'm here to help you find out! ðŸ”

        You can send me:
        ðŸ§¾ A political statement you want checked  
        ðŸŒ A link to a political news article
        ðŸ—£ï¸ A quote from a political debate or social media post

        âš¡ I focus **only on political topics** â€” elections, laws, politicians, government actions, policies, and more.

        ðŸ”Ž Go ahead â€” what political claim should we check today?

    3. Unrelated or off-topic input:
    - If the user asks about non-political topics (e.g., food, travel, personal advice), politely explain 
      that you specialize in political fact-checking and guide them to ask about political topics instead.
    âž” Example:
      Input: What's the best pizza place in New York?
      Response:
      "I'm here to help verify political claims, elections, government actions, and similar topics. ðŸ•
      For non-political questions like restaurant recommendations, I recommend using a general search engine.
      Feel free to ask me about political news, policies, or government actions!"


Response Format by Type:
"fact_check" â†’ Respond strictly with the keyword "fact_check".
"follow_up" â†’ Respond strictly with the keyword "follow_up".
"generic_response" â†’ Provide a full paragraph response.
"opinion_analysis" -> 

Summary:
- Use "fact_check" when fresh, external information is needed.
- Use "follow_up" when building upon prior discussion.
- Use "generic_response" for common knowledge, small talk, or when clarification is needed.
"""

def intent_detection(user_input: str, room_id: str, user_name: str):
    print("[intent_detection] module activated.")
    response = generate(
        model="4o-mini",
        system=INTENT_DETECTION_PROMPT,
        query=user_input,
        temperature=0.2,
        lastk=1,
        session_id=f"{SESSION}_{user_name}",
        rag_usage=False
    )
    print(f"[intent_detection] module ended.")
    return response["response"]


import random

if __name__ == "__main__":
    # Small set of mixed test cases
    test_cases = [
        ("fact_check", "Did the new immigration law pass last night?"),
        # ("follow_up", "What does that ruling mean for small businesses?"),
        ("generic_response", "Who is the President of Canada?"),
        ("generic_response", "voting rights"),
        ("generic_response", "hey there!"),
        ("mixed_paragraph", """There's been a lot of debate about election security lately. 
                               Some say it's improved, others say it's worse. What's the real deal?"""),
        ("generic_response", "Tell me a good movie to watch tonight")
    ]

    # Randomize order
    random.shuffle(test_cases)

    # Run tests
    for idx, (expected_category, user_input) in enumerate(test_cases, start=1):
        print(f"\n=== Test {idx} | Expected: {expected_category.upper()} ===")
        print(f"Input: {user_input}\n")
        room_id = "test_room"
        user_name = "test_user"
        response = intent_detection(user_input.strip(), room_id, user_name)
        print(f"Response: {response}")